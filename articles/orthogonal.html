<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Orthogonal Regression • nmmr</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Orthogonal Regression">
<meta property="og:description" content="nmmr">
<meta name="robots" content="noindex">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">nmmr</a>
        <span class="version label label-danger" data-toggle="tooltip" data-placement="bottom" title="Unreleased version">0.0.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/deming-regression.html">Bayesian Hierarchical Deming Regression</a>
    </li>
    <li>
      <a href="../articles/orthogonal.html">Orthogonal Regression</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/psadil/nmmr/">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="orthogonal_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Orthogonal Regression</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/psadil/nmmr/blob/master/vignettes/orthogonal.Rmd"><code>vignettes/orthogonal.Rmd</code></a></small>
      <div class="hidden name"><code>orthogonal.Rmd</code></div>

    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyr.tidyverse.org">tidyr</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://forcats.tidyverse.org">forcats</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://purrr.tidyverse.org">purrr</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://psadil.github.io/nmmr/">nmmr</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/tidyverse/glue">glue</a></span><span class="op">)</span>

<span class="va">size_shape</span> <span class="op">&lt;-</span> <span class="fl">2</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1234</span><span class="op">)</span></code></pre></div>
<div id="how-can-neuromodulation-be-assessed-quickly" class="section level1">
<h1 class="hasAnchor">
<a href="#how-can-neuromodulation-be-assessed-quickly" class="anchor"></a>How can neuromodulation be assessed quickly?</h1>
<p>A key insight of NMM is that, under certain circumstances, neuromodulation is reflected in changes to the voxel tuning function. That is, changes in the voxels tuning function implicate certain forms of neuromodulation. NMM outlines two techniques for looking at voxel tuning functions. One technique, a Bayesian modeling approach, requires assuming that the neural tuning functions and distributions of neurons within voxels follow a specific form. The Bayesian component of NMM pools information across voxels with hierarchical modeling, boiling down the question of neuromodulation into model comparison. But the Bayesian model assumes that the weight distribution and neural tuning function follow specific functional forms, assumptions that may not hold in all datasets (e.g., with some voxel sizes, the weight distribution may be multimodal). Moreover, the full Bayesian model is complex, making it challenging to assess which features of the data drive a comparison in favor of multiplicative gain versus additive offset. So, the Bayesian approach makes assumptions that may not always hold, is computationally expensive, and is hard to understand.</p>
<p>To complement the parametric modeling, NMM includes a non-parametric method for checking the form of neuromodulation, covered in this vignette.</p>
</div>
<div id="non-parametric-check" class="section level1">
<h1 class="hasAnchor">
<a href="#non-parametric-check" class="anchor"></a>Non-parametric Check</h1>
<p>Ideally, we could assess changes in the voxel tuning functions by simply plotting them. Generating that plot is easy enough. This package comes with the dataset <code>sub02</code>, which contains the responses for voxels in V1 for a single participant (author PS). Here are the tuning functions for six of those voxels.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R">
<span class="co"># take six voxels worth of data and calculate summary statistics</span>
<span class="va">six_voxels</span> <span class="op">&lt;-</span> <span class="va">sub02</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">contrast</span>, <span class="va">orientation</span>, <span class="va">run</span>, <span class="va">ses</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/slice.html">slice_head</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">6</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">voxel</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_nest.html">group_nest</a></span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>
    data <span class="op">=</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map</a></span><span class="op">(</span>
      <span class="va">data</span>, 
      <span class="op">~</span><span class="fu"><a href="../reference/WISEsummary.html">WISEsummary</a></span><span class="op">(</span><span class="va">.x</span>, dependentvars <span class="op">=</span> <span class="va">y</span>, withinvars <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">orientation</span>, <span class="va">contrast</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/nest.html">unnest</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span>

<span class="co"># plot the tuning functions</span>
<span class="va">six_voxels</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">orientation</span>, y<span class="op">=</span><span class="va">y_mean</span>, color <span class="op">=</span> <span class="va">contrast</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html">facet_wrap</a></span><span class="op">(</span><span class="op">~</span><span class="va">voxel</span>, labeller <span class="op">=</span> <span class="va">label_both</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_linerange.html">geom_errorbar</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>ymin<span class="op">=</span><span class="va">y_mean</span><span class="op">-</span><span class="va">y_sem</span>, ymax<span class="op">=</span><span class="va">y_mean</span><span class="op">+</span><span class="va">y_sem</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"Average BOLD Signal (% Average Signal)"</span><span class="op">)</span> </code></pre></div>
<div class="figure">
<img src="orthogonal_files/figure-html/vtfs-1.png" alt="Individual voxel tuning functions are too noisy to provide information about neuromodulation individually. Error bars span 95% confidence intervals" width="700"><p class="caption">
Individual voxel tuning functions are too noisy to provide information about neuromodulation individually. Error bars span 95% confidence intervals
</p>
</div>
<p>Although intuitive, plots of the voxel tuning function are uninformative. This is because most voxels have weak tuning. That is, across orientations, the lines are nearly flat, and the activity is rather variable. When tuning is weak, different forms of neuromodulation look identical (i.e., multiplying a flat line by a gain factor can produce the same result as simply adding an offset). So, simply plotting the voxel tuning functions does not offer much insight into the form of neuromodulation.</p>
<div id="a-scatter-plot-of-activity-across-conditions-reveals-changes-in-voxel-tuning" class="section level2">
<h2 class="hasAnchor">
<a href="#a-scatter-plot-of-activity-across-conditions-reveals-changes-in-voxel-tuning" class="anchor"></a>A scatter plot of activity across conditions reveals changes in voxel tuning</h2>
<p>The central idea, detailed in the main paper, is that we can assess multiplicative vs. additive shift by looking at the slope of a line made from the low-contrast activity plotted against the high-contrast activity. That is, a key difference between the models we compared is that the multiplicative model allows the effect of contrast to vary by orientation whereas the additive model does not. Additive neuromodulation corresponds to an increase in neural activity at all orientations. Thus, regardless of the forms of the weight distribution and neural tuning function, additive neuromodulation causes the low-contrast tuning function to shift upwards uniformly across orientations. Hence, a scatter plot of the voxel’s response to high-contrast stimuli against its response to low-contrast stimuli has a slope of 1. In contrast, multiplicative neuromodulation corresponds to an increase in neural activity at the most preferred orientations, and a scatter plot of a voxel’s response to high versus low-contrast stimuli has a slope larger than 1. Therefore, the models can be differentiated by plotting high-contrast activity against low-contrast activity and calculating the slope of the best fitting line. A slope of one implies additive shift, but a slope greater than one implies multiplicative gain.</p>
<div class="figure">
<img src="orthogonal_files/figure-html/add_mult-1.png" alt="Plotting beta a voxel's response to high- versus low-contrast uncovers neuromodulation. Left: Simulated voxel tuning functions in which higher levels of contrast induce either an additive (top) or multiplicative (bottom) neuromodulation. The eight vertical lines are eight hypothetical orientations at which these voxel tuning functions might be probed, which would produce eight responses per level of contrast. Right: The two kinds of neuromodulation reveal different signatures when the response to high-contrast stimuli are plotted against the response to low-contrast stimuli. The diagonal line corresponds to no effect of contrast. A line drawn through the points produced by the additive model necessarily has a slope equal to 1; under this neuromodulation, the effect of contrast does not depend on the orientation. A line drawn through the points produced by the multiplicative model necessarily has a slope greater than 1; under this neuromodulation, the effect of contrast is largest at those orientations which are closest to the voxel's preferred orientation." width="700"><p class="caption">
Plotting beta a voxel’s response to high- versus low-contrast uncovers neuromodulation. Left: Simulated voxel tuning functions in which higher levels of contrast induce either an additive (top) or multiplicative (bottom) neuromodulation. The eight vertical lines are eight hypothetical orientations at which these voxel tuning functions might be probed, which would produce eight responses per level of contrast. Right: The two kinds of neuromodulation reveal different signatures when the response to high-contrast stimuli are plotted against the response to low-contrast stimuli. The diagonal line corresponds to no effect of contrast. A line drawn through the points produced by the additive model necessarily has a slope equal to 1; under this neuromodulation, the effect of contrast does not depend on the orientation. A line drawn through the points produced by the multiplicative model necessarily has a slope greater than 1; under this neuromodulation, the effect of contrast is largest at those orientations which are closest to the voxel’s preferred orientation.
</p>
</div>
<div id="orthogonal-regression" class="section level3">
<h3 class="hasAnchor">
<a href="#orthogonal-regression" class="anchor"></a>Orthogonal Regression</h3>
<p>Based on the above, we know that different forms of neuromodulation are implicated by the slope of a line fit to high- versus low-contrast activity. However, to estimate this line, we should not estimate the line with ordinary least squares. This subsection will overview why ordinary least squares is insufficient, suggesting instead orthogonal regression <a href="https://en.wikipedia.org/wiki/Deming_regression#Orthogonal_regression">orthogonal regression</a>.</p>
</div>
<div id="sample-data" class="section level3">
<h3 class="hasAnchor">
<a href="#sample-data" class="anchor"></a>Sample Data</h3>
<p>Here is a toy example to build intuition for the failings of ordinary least squares as an estimate of slope in this situation. Let the variable <span class="math inline">\(z\)</span> refer to the average activity of a voxel at low contrast. For simplicity, assume that this variable is distributed according to a standard normal distribution.</p>
<p><span class="math display">\[
z \sim N(0, 1)
\]</span></p>
<p>As researchers, we are unable to directly observe <span class="math inline">\(z\)</span>; this variable is the <em>average</em> activity at low contrast, but on any given trial a voxel’s response varies around the average. Assume that the variability follows another standard normal distribution, so the resulting observed activity at low contrast, <span class="math inline">\(x\)</span>, is distributed according to another normal distribution</p>
<p><span class="math display">\[
x \sim N(z, 1)
\]</span></p>
<p>The activity at high contrast will be a function of the activity at high contrast. In this toy example, let’s assume multiplicative gain factor of two. Importantly, the activity at high contrast will be a function of the latent voxel tuning function, <span class="math inline">\(z\)</span>, not the noise-corrupted observation, <span class="math inline">\(x\)</span>. Activity at high contrast, <span class="math inline">\(y\)</span>, will then be distributed according to the following (again, assuming standard normal noise).</p>
<p><span class="math display">\[
y \sim N(2z, 1)
\]</span></p>
<p>Together, these assumptions define a model from which we can simulate datasets. The following chunk defines a function for simulating from this toy model and then uses it to generate 1000 observations (i.e., this would be 1000 observations for a single voxel at each of low and high contrast).</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">simulate_toy</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">N</span>, <span class="va">true_slope</span> <span class="op">=</span> <span class="fl">2</span><span class="op">)</span><span class="op">{</span>
  <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span>z <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span>
    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>
      x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span>, <span class="va">z</span><span class="op">)</span>,
      y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span>, <span class="fl">2</span><span class="op">*</span><span class="va">z</span><span class="op">)</span><span class="op">)</span>
<span class="op">}</span>

<span class="va">d</span> <span class="op">&lt;-</span> <span class="fu">simulate_toy</span><span class="op">(</span>N <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span></code></pre></div>
</div>
<div id="ordinary-least-squares-will-not-work" class="section level3">
<h3 class="hasAnchor">
<a href="#ordinary-least-squares-will-not-work" class="anchor"></a>Ordinary Least Squares will not work</h3>
<p>We can use the built-in function <code>lm</code> to estimate the slope with ordinary least squares.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">ls</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, data<span class="op">=</span><span class="va">d</span><span class="op">)</span>
<span class="va">ls</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; lm(formula = y ~ x, data = d)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt; (Intercept)            x  </span>
<span class="co">#&gt;    -0.01208      1.00627</span></code></pre></div>
<p>At only about 1.01, the estimated slope is only about half of the true slope (2)! The following plot shows the difference between the line we wanted to estimate (with a slope of 2), versus the line provided by <code>lm</code>.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">d</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">x</span>, y<span class="op">=</span><span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">.2</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_abline</a></span><span class="op">(</span>intercept<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">ls</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, slope<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">ls</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, color<span class="op">=</span><span class="st">"blue"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/coord_fixed.html">coord_fixed</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="fu"><a href="https://glue.tidyverse.org/reference/glue.html">glue</a></span><span class="op">(</span><span class="st">"OLS slope: {round(coef(ls)[2], digits=2)}"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_abline</a></span><span class="op">(</span>intercept<span class="op">=</span><span class="fl">0</span>, slope<span class="op">=</span><span class="fl">2</span>, linetype <span class="op">=</span> <span class="st">"dashed"</span>, color<span class="op">=</span><span class="st">"blue"</span><span class="op">)</span></code></pre></div>
<div class="figure">
<img src="orthogonal_files/figure-html/tls_plot-1.png" alt="We want to estimate the dashed line, which has a slope of 2. However, Ordinary Least Squares (OLS) provides an estimate that is biased towards 0 (solid line)." width="700"><p class="caption">
We want to estimate the dashed line, which has a slope of 2. However, Ordinary Least Squares (OLS) provides an estimate that is biased towards 0 (solid line).
</p>
</div>
<p>This issue is known as <a href="https://en.wikipedia.org/wiki/Regression_dilution">Regression Dilution</a>. One way to understand the issue that that there is variability in, not just the y-coordinate, but also the x-coordinate, which inspires the name <a href="https://en.wikipedia.org/wiki/Errors-in-variables_models">errors-in-variables model</a><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. For these circumstances, we need to use a <em>orthogonal</em> regression.</p>
</div>
<div id="orthogonal-regression-accurately-recovers-our-desired-slope" class="section level3">
<h3 class="hasAnchor">
<a href="#orthogonal-regression-accurately-recovers-our-desired-slope" class="anchor"></a>Orthogonal regression accurately recovers our desired slope</h3>
<p>The <code>nmmr</code> package provides a function to estimate the slope, called <code>get_slope</code>. It takes two vectors of numbers. Here is how to apply it on our toy example</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">slope</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/get_slope.html">get_slope</a></span><span class="op">(</span><span class="va">d</span><span class="op">$</span><span class="va">x</span>, <span class="va">d</span><span class="op">$</span><span class="va">y</span><span class="op">)</span>
<span class="va">slope</span>
<span class="co">#&gt; [1] 1.952462</span></code></pre></div>
<p>This is much closer to the true slope!</p>
<p>But perhaps we got lucky. Let’s stimulate 1000 datasets, estimating for each the slope with both ordinary least squares and orthogonal regression.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">fits</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tidyr.tidyverse.org/reference/expand.html">crossing</a></span><span class="op">(</span>i <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">1000</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>simulation <span class="op">=</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map</a></span><span class="op">(</span><span class="va">i</span>, <span class="va">simulate_toy</span>, N<span class="op">=</span><span class="fl">1000</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/rowwise.html">rowwise</a></span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>
  ordinary <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, data<span class="op">=</span><span class="va">simulation</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">magrittr</span><span class="fu">::</span><span class="fu"><a href="https://magrittr.tidyverse.org/reference/aliases.html">extract</a></span><span class="op">(</span><span class="fl">2</span><span class="op">)</span>,
  orthogonal <span class="op">=</span> <span class="fu"><a href="../reference/get_slope.html">get_slope</a></span><span class="op">(</span><span class="va">simulation</span><span class="op">$</span><span class="va">x</span>, <span class="va">simulation</span><span class="op">$</span><span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">simulation</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">pivot_longer</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">ordinary</span>, <span class="va">orthogonal</span><span class="op">)</span>, names_to <span class="op">=</span> <span class="st">"Method"</span>, values_to <span class="op">=</span> <span class="st">"Estimate"</span><span class="op">)</span></code></pre></div>
<p>Next, plot the distribution of recovered slopes for each of the two methods.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">fits</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">Estimate</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>fill<span class="op">=</span><span class="va">Method</span><span class="op">)</span>, bins <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_vline</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fl">2</span>, linetype <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span></code></pre></div>
<div class="figure">
<img src="orthogonal_files/figure-html/ls_compare-1.png" alt="Across many simulated datasets, the ordinary least squares method estimates a slope that is too small, whereas the orthogonal regression is unbiased. The dashed vertical line marks the true slope." width="700"><p class="caption">
Across many simulated datasets, the ordinary least squares method estimates a slope that is too small, whereas the orthogonal regression is unbiased. The dashed vertical line marks the true slope.
</p>
</div>
<p>These simulations suggest that ordinary least squares is indeed biased, whereas orthogonal regression gives a more accurate slope.</p>
</div>
</div>
<div id="application-to-data" class="section level2">
<h2 class="hasAnchor">
<a href="#application-to-data" class="anchor"></a>Application to data</h2>
<p>We can apply this same method to estimate a slope for each voxel in the provided dataset, <code>sub02</code>. To use <code>get_slope</code>, first <a href="https://r4ds.had.co.nz/tidy-data.html">widen</a> the data.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">sub02_wide</span> <span class="op">&lt;-</span> <span class="va">sub02</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_wider.html">pivot_wider</a></span><span class="op">(</span>names_from <span class="op">=</span> <span class="va">contrast</span>, values_from <span class="op">=</span> <span class="va">y</span><span class="op">)</span>

<span class="va">sub02_wide</span>
<span class="co">#&gt; # A tibble: 26,928 x 7</span>
<span class="co">#&gt;    sub   run   voxel  orientation ses      low    high</span>
<span class="co">#&gt;    &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;        &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;   &lt;dbl&gt;</span>
<span class="co">#&gt;  1 2     15    191852       0.785 3      3.36   6.39  </span>
<span class="co">#&gt;  2 2     15    197706       0.785 3     -2.84   0.0160</span>
<span class="co">#&gt;  3 2     15    197769       0.785 3     -2.03   4.14  </span>
<span class="co">#&gt;  4 2     15    197842       0.785 3      2.23   2.35  </span>
<span class="co">#&gt;  5 2     15    197906       0.785 3      2.86   2.11  </span>
<span class="co">#&gt;  6 2     15    197907       0.785 3      1.51   0.504 </span>
<span class="co">#&gt;  7 2     15    203688       0.785 3     -2.75   0.241 </span>
<span class="co">#&gt;  8 2     15    203689       0.785 3     -2.51  -2.34  </span>
<span class="co">#&gt;  9 2     15    203690       0.785 3     -3.16  -1.40  </span>
<span class="co">#&gt; 10 2     15    203753       0.785 3      0.711  0.427 </span>
<span class="co">#&gt; # … with 26,918 more rows</span></code></pre></div>
<p>As mentioned above, contrast seems to have a different effect on each voxel. Hence, we should calculate the slope for each voxel separately. The <code>nmmr</code> package provides a helper function for applying the <code>get_slope</code> function to groups of a dataset, called <code>get_slope_by_group</code>.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">slopes</span> <span class="op">&lt;-</span> <span class="va">sub02_wide</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="../reference/get_slope_by_group.html">get_slope_by_group</a></span><span class="op">(</span><span class="va">voxel</span>, <span class="va">low</span>, <span class="va">high</span><span class="op">)</span></code></pre></div>
<p>For visualization, plot the distribution of estimated slopes.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R">
<span class="va">slopes</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">slope</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span><span class="op">(</span>bins <span class="op">=</span> <span class="fl">200</span><span class="op">)</span> </code></pre></div>
<div class="figure">
<img src="orthogonal_files/figure-html/voxel_slopes-1.png" alt="When looking at all of the voxels, some of the slopes are spuriously high." width="700"><p class="caption">
When looking at all of the voxels, some of the slopes are spuriously high.
</p>
</div>
<p>Unfortunately, this distribution of slopes is not helpful. Clearly, there are outliers (e.g., one voxel has a slope that is over 200). These outliers are symptomatic of two challenges identified at the beginning of this voxel: voxels are flat and noisy. We can see the issue by looking at the activity for a single voxel.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">sub02_wide</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="fu"><a href="https://forcats.tidyverse.org/reference/fct_match.html">fct_match</a></span><span class="op">(</span><span class="va">voxel</span>, <span class="st">"209671"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">low</span>, y <span class="op">=</span> <span class="va">high</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/coord_fixed.html">coord_fixed</a></span><span class="op">(</span><span class="op">)</span> </code></pre></div>
<div class="figure">
<img src="orthogonal_files/figure-html/circle-1.png" alt="When voxels are poorly tuned, the best-fitting line can easily be one that is nearly vertical." width="700"><p class="caption">
When voxels are poorly tuned, the best-fitting line can easily be one that is nearly vertical.
</p>
</div>
<p>When a voxel is tuned weakly, the scatter plot is nearly circular. When the data are circular, a line at any angle fits equally well. So, with circular scatter plots, orthogonal regression often produces slopes that have a spuriously high slope.</p>
</div>
<div id="thresholding" class="section level2">
<h2 class="hasAnchor">
<a href="#thresholding" class="anchor"></a>Thresholding</h2>
<p>Orthogonal least squares allows us to focus on the form of neuromodulation, but now we need a way to focus on just those voxels that are responsive to stimulation. In the paper, we advise thresholding based on each voxel’s responsivity to contrast. That is, we advise considering each voxel’s average difference in activity between high and low contrast, and then selecting only those voxels whose differences are in the upper quantiles.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">sub02_wide</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>diff <span class="op">=</span> <span class="va">high</span> <span class="op">-</span> <span class="va">low</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">voxel</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarise</a></span><span class="op">(</span>average_difference <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">diff</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span><span class="op">(</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">average_difference</span><span class="op">)</span>,
    bins <span class="op">=</span> <span class="fl">20</span><span class="op">)</span></code></pre></div>
<div class="figure">
<img src="orthogonal_files/figure-html/avg_diff-1.png" alt="Distribution of average difference between high and low contrast, across voxels. The thresholding is based on the quantiles of this distribution." width="700"><p class="caption">
Distribution of average difference between high and low contrast, across voxels. The thresholding is based on the quantiles of this distribution.
</p>
</div>
<p>In the paper, we looked at quatiles of 0 (i.e., no thresholding, analyze all voxels), and 0.9 (i.e., select only the top 10% of voxels within a participant). The <code>nmmr</code> function <code>cross_threshold</code> takes a dataframe and a vector of quantiles, and it returns another dataframe that can be used to filter voxels in the original data.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">ranks</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cross_threshold.html">cross_threshold</a></span><span class="op">(</span><span class="va">sub02_wide</span>, <span class="va">voxel</span>, <span class="va">low</span>, <span class="va">high</span>, quantiles <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0.9</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>Joining the returned dataframe with the original dataset allows us to plot the distribution of slopes at both thresholds.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate-joins.html">left_join</a></span><span class="op">(</span><span class="va">ranks</span>, <span class="va">slopes</span>, by <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"voxel"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">slope</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html">facet_wrap</a></span><span class="op">(</span><span class="op">~</span><span class="va">Threshold</span>, scales <span class="op">=</span> <span class="st">"free"</span>, labeller <span class="op">=</span> <span class="va">label_both</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span><span class="op">(</span>bins <span class="op">=</span> <span class="fl">20</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_vline</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></code></pre></div>
<div class="figure">
<img src="orthogonal_files/figure-html/thresholded_slopes-1.png" alt="Thresholding based on the average difference across levels of contrast removed the spuriously high slopes. Vertical line marks a slope of 1, which is predicted by the additive but not multiplicative model." width="700"><p class="caption">
Thresholding based on the average difference across levels of contrast removed the spuriously high slopes. Vertical line marks a slope of 1, which is predicted by the additive but not multiplicative model.
</p>
</div>
<p>This need to threshold is <em>ad hoc</em>; there is no clear reason why we should look at the top 10% of voxels, rather than the top 20% or 5%. But the point of this orthogonal regression is not to produce a precise measure of neuromodulation. Instead, it is meant as a rough-and-ready tool for checking whether neuromodulation is additive or multiplicative. In the <code>nmmr</code> package, the main functions for performing these checks are <code>get_slope</code>, <code>get_slope_by_group</code>, and <code>cross_threshold</code>.</p>
<p>If you need a method that is more quantitative, <code>nmmr</code> provides two other sets of functions, each covered in their own vignette. The most complex is the full Bayesian model. As outlined above, the Bayesian method uses hierarchical modeling to pool information across voxels. However, it also makes assumptions that may not hold in all datasets. For this reason, <code>nmmr</code> includes an intermediate approach, which implements the orthogonal regression idea in a Bayesian hierarchical model This has the advantage of still being quick (relative to the main Bayesian approach), but does not require <em>ad hoc</em> thresholding.</p>
<p>The intermediate approach is covered in <code><a href="../articles/deming-regression.html">vignette("deming-regression")</a></code></p>
</div>
</div>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p>Put another way, ordinary least squares assumes a model like <span class="math inline">\(y\simN(2x, 1)\)</span> – e.g., that the activity at high contrast is directly related to the activity at low contrast<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Patrick Sadil.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
