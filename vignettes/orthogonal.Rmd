---
title: "orthogonal"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{orthogonal}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE)

if (requireNamespace("ragg", quietly = TRUE)) knitr::opts_chunk$set(dev = "ragg_png")
```


```{r setup, message=FALSE}
library(dplyr)
library(ggplot2)
library(tidyr)
library(forcats)
library(purrr)
library(nmmr)
library(glue)

size_shape <- 2
set.seed(1234)
```

# Introduction

A key insight of NMM is that, under certain circumstances, neuromodulation is reflected in changes to the voxel tuning function. That is, changes in the voxels tuning function implicate certain forms of neuromodulation. NMM outlines two techniques for looking at voxel tuning functions. One technique requires assuming that the neural tuning functions and distributions of neurons within voxels follow a specific form. The other technique makes fewer assumptions and is a bit simpler to run. This vignette covers the latter technique.

# The Challenge: Voxels are too noisy

Ideally, we could assess changes in the voxel tuning function by simply plotting them. That is an easy enough plot to generate. This package comes with the dataset `sub02`, which contains the responses for voxels in V1 for a single participant (author PS). Here are the tuning functions for six of those voxels.

```{r, vtfs, fig.cap="Individual voxel tuning functions are too noisy to provide information about neuromodulation individually. Error bars span 95% confidence intervals"}

# take six voxels worth of data and calculate summary statistics
six_voxels <- sub02 %>%
  group_by(contrast, orientation, run, ses) %>%
  slice_head(n=6) %>%
  group_by(voxel) %>%
  group_nest() %>%
  mutate(
    data = map(
      data, 
      ~WISEsummary(.x, dependentvars = y, withinvars = c(orientation, contrast)))) %>%
  unnest(data)

# plot those statistics
six_voxels %>%
  ggplot(aes(x=orientation, y=y_mean, color = contrast)) +
  facet_wrap(~voxel, labeller = label_both) +
  geom_line() +
  geom_errorbar(aes(ymin=y_mean-y_sem, ymax=y_mean+y_sem)) +
  ylab("Average BOLD Signal (% Average Signal)") 
```

Unfortunately, most voxels have weak tuning. That is, across orientations, the lines are nearly flat. Without tuning, different forms of neuromodulation are conflated; multiplicative gain looks like an additive shift. Moreover, in even those voxels that are tuned, their activity is rather variable. Variable activity makes it challenging to distinguish between different forms of neuromodulation within a single voxel. A simple approach would involve collapsing across voxels. But such collapsing would ignore the clear differences in how voxels respond to the experimental manipulation (i.e., how they respond to contrast).

So, there are two challenges. First, we need an analysis that focuses on the form of neuromodulation but which does not require sharp tuning. Second, we need that this analysis can pool information across voxels, while also being sensitive to differences in the voxels' responsivity. 

# Solutions

## A scatterplot of activity across conditions reveals changes in voxel tuning

First, we need some way to focus on the neuromodulation itself, rather than the tuning. The central idea, detailed in the main paper, is that we can assess multiplicative vs. additive shift by looking at the slope of a line made from the low-contrast tuning plotted against the high-contrast tuning.

A key difference between the models we compared is that the multiplicative model allows the effect of contrast to vary by orientation whereas the additive model does not. Additive neuromodulation corresponds to an increase in neural activity at all orientations. Thus, regardless of the forms of the weight distribution and neural tuning function, additive neuromodulation causes the low-contrast tuning function to shift upwards uniformly across orientations. Hence, a scatterplot of the voxel's response to high-contrast stimuli against its response to low-contrast stimuli has a slope of 1. In contrast, multiplicative neuromodulation corresponds to an increase in neural activity at the most preferred orientations, and a scatterplot of a voxel's response to high versus low-contrast stimuli has a slope larger than 1. Therefore, the models can be differentiated by plotting high-contrast activity against low-contrast activity and calculating the slope of the best fitting line. A slope of one implies additive shift, but a slope greater than one implies multiplicative gain.

```{r, add_mult, echo=FALSE, fig.cap="Plotting beta a voxel's response to high- versus low-contrast uncovers neuromodulation. Left: Simulated voxel tuning functions in which higher levels of contrast induce either an additive (top) or multiplicative (bottom) neuromodulation. The eight vertical lines are eight hypothetical orientations at which these voxel tuning functions might be probed, which would produce eight responses per level of contrast. Right: The two kinds of neuromodulation reveal different signatures when the response to high-contrast stimuli are plotted against the response to low-contrast stimuli. The diagonal line corresponds to no effect of contrast. A line drawn through the points produced by the additive model necessarily has a slope equal to 1; under this neuromodulation, the effect of contrast does not depend on the orientation. A line drawn through the points produced by the multiplicative model necessarily has a slope greater than 1; under this neuromodulation, the effect of contrast is largest at those orientations which are closest to the voxel's preferred orientation."}
library(patchwork)
make_d <- function(raw,
                   kv = 2,
                   center = -pi/16,
                   m = 1.4,
                   a = 0.1,
                   p = 0.6) {
  d <- tibble(raw = raw) %>%
    crossing(
      Contrast = factor(c("Low", "High"), levels = c("Low", "High")),
      VTF = factor(
        c("Additive", "Multiplicative"),
        levels = c("Additive", "Multiplicative"))) %>%
    mutate(
      y = case_when(
        fct_match(Contrast, "Low") ~ CircStats::dmixedvm(raw, center, center+pi, kv, kv, p),
        fct_match(VTF, "Additive") ~ a + CircStats::dmixedvm(raw, center, center+pi, kv, kv, p),
        fct_match(VTF, "Multiplicative") ~ m*CircStats::dmixedvm(raw, center, center+pi, kv, kv, p)),
      orientation = CircStats::deg(raw / 2))
  return(d)
}

test_oris <- make_d(seq(-pi, pi-2*pi/8, length.out = 8)) %>%
  group_by(orientation, VTF) %>%
  mutate(
    high = max(y),
    higher = high + 0.025,
    highest = 0.6) %>%
  filter(fct_match(Contrast, "Low"))

a <- make_d(seq(-pi, pi, length.out = 1000)) %>%
  ggplot(aes(x=orientation)) +
  facet_wrap(~VTF, nrow=2, strip.position = "left") +
  geom_segment(
    aes(
      xend = orientation,
      y = y,
      yend = high),
    data = test_oris,
    show.legend = FALSE,
    size = .4) +
  geom_point(
    aes(
      x = orientation,
      y = highest,
      shape = factor(orientation)),
    data = test_oris %>%filter(fct_match(VTF, "Multiplicative")),
    show.legend = FALSE,
    size = size_shape) +
  geom_segment(
    aes(
      xend = orientation,
      y = 0),
    yend = 0.6,
    alpha = 0.25,
    data = test_oris,
    size = 0.25) +
  geom_line(
    aes(
      y = y,
      linetype = Contrast)) +
  scale_x_continuous(
    name = "Orientation",
    breaks = seq(-90, 90, length.out = 3),
    labels = seq(-90, 90, length.out = 3)) +
  scale_y_continuous(
    name = expression(beta),
    labels = NULL,
    breaks = NULL) +
  coord_cartesian(ylim = c(0, 0.6)) +
  scale_color_viridis_c(option = "inferno", end = .8) +
  scale_linetype_manual(values = c("solid", "dashed")) +
  scale_shape_manual(values = c(1:8)) +
  theme_classic(base_size = 10) +
  theme(
    axis.line.y = element_blank(),
    rect = element_blank(),
    legend.position = "bottom")

b <- make_d(seq(-pi, pi-2*pi/8, length.out = 8)) %>%
  group_by(VTF, orientation) %>%
  mutate(avg = mean(y)) %>%
  group_by(VTF, Contrast) %>%
  mutate(rank = rank(y)) %>%
  ungroup() %>%
  pivot_wider(names_from = Contrast, values_from = y) %>%
  ggplot(aes(x = Low, y = High)) +
  facet_wrap(~VTF, nrow = 2) +
  geom_abline(
    slope = 1,
    intercept = 0) +
  geom_line(alpha = 0.25) +
  geom_point(
    aes(shape = factor(orientation)),
    show.legend = FALSE,
    size = size_shape) +
  scale_x_continuous(
    name = expression(paste("Low Contrast ", beta)),
    labels = NULL,
    breaks = NULL) +
  scale_y_continuous(
    name = expression(paste("High Contrast ", beta)),
    labels = NULL,
    breaks = NULL) +
  coord_cartesian(xlim = c(0,0.5), ylim = c(0,0.5)) +
  scale_color_viridis_c(option = "inferno", end = .8) +
  scale_linetype_manual(values = c("dotted", "dashed")) +
  scale_shape_manual(values = 1:8) +
  theme_classic(base_size = 10) +
  theme(
    rect = element_blank(),
    legend.position = "none",
    strip.text = element_blank()) +
  annotate("segment", x=-Inf, xend=Inf, y=-Inf, yend=-Inf, size = 0.5)


a + b + plot_layout(nrow=1)


```


### Orthogonal Regression

Based on the above, we know that different forms of neuromodulation are implicated by the slope of a line fit to high- versus low-contrast activity. However, to estimate this line, we should not estimate the line with ordinary least squares. This subsection will overview why ordinary least squares is insufficient, suggesting instead orthogonal regression [orthogonal regression](https://en.wikipedia.org/wiki/Deming_regression#Orthogonal_regression).

### Sample Data

Here is a toy example to build intuition for the failings of ordinary least squares as an estimate of slope in this situation. Let the variable $z$ refer to the average activity of a voxel at low contrast. For simplicity, assume that this variable is distributed according to a standard normal distribution.

$$
z \sim N(0, 1)
$$

As researchers, we are unable to directly observe $z$; this variable is the _average_ activity at low contrast, but on any given trial a voxels response varies around the average. Assume that the variability follows another standard normal distribution, so the resulting observed activity at low contrast, $x$, is distributed according to another normal distribution

$$
x \sim N(z, 1)
$$

The activity at high contrast will be a function of the activity at high contrast. In this toy example, let's assume multiplicative gain factor of two. Importantly, the activity at high contrast will be a function of the latent voxel tuning function, $z$, not the noise-corrupted observation, $x$. Activity at high contrast, $y$, will then be distributed according to the following (again, assuming standard normal noise).

$$
y \sim N(2z, 1)
$$

Together, these assumptions define a model from which we can simulate datasets. The following chunk defines a function for simulating from this toy model and then uses it to generate 1000 observations (i.e., this would be 1000 observations for a single voxel at each of low and high contrast). 

```{r, tls_data}
simulate_toy <- function(N, true_slope = 2){
  tibble(z = rnorm(N)) %>%
    mutate(
      x = rnorm(N, z),
      y = rnorm(N, 2*z))
}

d <- simulate_toy(N = 1000)

```


### Ordinary Least Squares will not work

We can use the built-in function `lm` to estimate the slope with ordinary least squares.

```{r, ols_slope}
ls <- lm(y ~ x, data=d)
ls
```
At only about `r round(coef(ls)[2], digits=2)`, the estimated slope is only about half of the true slope (2)! The following plot shows the difference between the line we wanted to estimate (with a slope of 2), versus the line provided by `lm` (a slope of ~1).

```{r, tls_plot, fig.cap="We want to estimate the dashed line, which has a slope of 2. However, Ordinary Least Squares (OLS) provides an estimate that is biased towards 0 (solid line)."}
d %>%
  ggplot(aes(x=x, y=y)) +
  geom_point(alpha = .2) +
  geom_abline(intercept=coef(ls)[1], slope=coef(ls)[2], color="blue") +
  coord_fixed() +
  ggtitle(glue("OLS slope: {round(coef(ls)[2], digits=2)}")) +
  geom_abline(intercept=0, slope=2, linetype = "dashed", color="blue")

```

This issue is known as [Regression Dilution](https://en.wikipedia.org/wiki/Regression_dilution). One way to understand the issue that that there is variability in, not just the y-coordinate, but also the x-coordinate, which inspires the name [errors-in-variables model](https://en.wikipedia.org/wiki/Errors-in-variables_models). For these circumstances, we need to use orthogonal regression.

### Orthogonal regression accurately recovers the slope

The `nmmR` package provides a function to estimate the slope, called `get_slope`. It takes two vectors of numbers. Here is how to apply it on our toy example

```{r, tls_slope}
slope <- get_slope(d$x, d$y)
slope
```
This is pretty much closer to the true slope!

But perhaps we got lucky. Let's stimulate 1000 datasets, estimating for each the sleop with ordinary least squares and orthogonal regression.

```{r, tls_show}
fits <- crossing(i = 1:1000) %>%
  mutate(simulation = map(i, simulate_toy, N=1000)) %>%
  rowwise() %>%
  mutate(
  ordinary = lm(y ~ x, data=simulation) %>% coef() %>% magrittr::extract(2),
  orthogonal = get_slope(simulation$x, simulation$y)) %>%
  select(-simulation) %>%
  pivot_longer(c(ordinary, orthogonal), names_to = "Method", values_to = "Estimate")
```

Next, plot the distribution of recovered slopes for each of the two methods.

```{r, ls_compare, fig.cap="Across many simulated datasets, the ordinary least squares method estimates a slope that is too small, whereas the orthogonal regression is unbiased. The dashed vertical line marks the true slope."}
fits %>%
  ggplot(aes(x=Estimate)) +
  geom_histogram(aes(fill=Method), bins = 100) +
  geom_vline(xintercept = 2, linetype = "dashed")
```

These simulations suggest that ordinary least squares is indeed biased, whereas orthogonal regression gives a more accurate slope.

## Application to data

We can apply this same method to estimate a slope for each voxel in the provided dataset, `sub02`. However, to sue `get_slope`, it would help if the data were [wider](https://r4ds.had.co.nz/tidy-data.html). 

```{r, pivot}

sub02_wide <- sub02 %>%
  pivot_wider(names_from = contrast, values_from = y)

sub02_wide
```

With these data, we want to estimate a single slope for each voxel. The `nmmR` package provides a helper function for applying the `get_slope` function to groups of a dataset, called `get_slope_by_group`. 

```{r, group}
slopes <- sub02_wide %>%
  get_slope_by_group(voxel, low, high)
```

For visualization, plot the distribution of estimated slopes.

```{r, voxel_slopes, fig.cap="When looking at all of the voxels, some of the slopes are spuriously high."}

slopes %>%
  ggplot(aes(x=slope)) +
  geom_histogram(bins = 200) 

```

Unfortunately, this distribution of slopes is not helpful. Clearly, there are outliers (one voxel has a slope of over 200). These outliers are symptomatic of the two problems identified at the beginning of this voxel: voxels are flat and noisy. We can see the issue by looking at the activity for a single voxel.

```{r, circle, fig.cap="When voxels are poorly tuned, the best-fitting line can easily be one that is nearly vertical."}

sub02_wide %>%
  filter(fct_match(voxel, "209671")) %>%
  ggplot(aes(x=low, y = high)) +
  geom_point() +
  coord_fixed() 

```

This voxel was only weakly tuned, and so the scatterplot is nearly circular. When the data are circular, a line at any angle fits equally well. With circular scatterplots, orthogonal regression produces slopes that have a spuriously high slope. 

## Thresholding

Orthogonal least squares allows us to focus on the form of neuromodulation, but now we need a way to focus on just those voxels that are responsive to stimulation. We advise thresholding based on each voxel's responsivity to contrast. That is, we advise considering each voxel's average difference in activity between high and low contrast, and then selecting only those voxels whose differences are in the upper quantiles. 

```{r avg_diff, fig.cap="Average difference between high and low contrast, across voxels."}
sub02_wide %>%
  mutate(diff = high - low) %>%
  group_by(voxel) %>%
  summarise(average_difference = mean(diff)) %>%
  ggplot() +
  geom_histogram(
    aes(x=average_difference),
    bins = 20)
```

In the paper, we looked at quatiles of 0 (i.e., no thresholding, analyze all voxels), and 0.9 (i.e., select only the top 10% of voxels within a participant). The `nmmR` function `cross_threshold` takes a dataframe and a vector of quantiles, and it returns another dataframe that can be used to filter voxels in the original data.

```{r, calc_ranks}
ranks <- cross_threshold(sub02_wide, voxel, low, high, quantiles = c(0, 0.9))
```

Joining the returned dataframe with the original dataset allows us to plot the distribution of slopes at both thresholds.

```{r, thresholded_slopes, fig.cap="Thresholding based on the average difference across levels of contrast removed the spuriously high slopes. Vertical line marks a slope of 1, which is predicted by the additive but not multiplicative model."}
left_join(ranks, slopes, by = c("voxel")) %>%
  ggplot(aes(x=slope)) +
  facet_wrap(~Threshold, scales = "free", labeller = label_both) +
  geom_histogram(bins = 20) +
  geom_vline(xintercept = 1)
```

This need to threshold is _ad hoc_; there is no clear reason why we should look at the top 10% of voxels, rather than the top 20% or 5%. But the point of this orthogonal regression is not to produce a precise measure of neuromodulation. Instead, it is meant as a rough-and-ready tool for getting a sense of whether neuromodulation might be present. With just three simple functions, `get_slope`, `get_slope_by_group`, and `cross_threshold`, you can get a sense of whether the manipulation causes an additive versus multiplicative gain.
 
If you need a method that is more quantitative, `nmmR` provides two other sets of functions, each covered in their own vignette. One set implements the orthogonal regression idea in a Bayesian hierarchical modeling framework. This has the advantage of still being relatively quick, but does not require _ad hoc_ thresholding. The other set is the main Bayesian model. 

